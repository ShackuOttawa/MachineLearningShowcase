{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CSI 4106 Introduction to Artificial Intelligence** <br/>\n",
        "*Assignment 1: Data Preparation*\n",
        "\n",
        "# Identification\n",
        "\n",
        "Name: Shacha Parker<br/>\n",
        "Student Number: 300235525\n",
        "\n",
        "# Exploratory Analysis\n",
        "\n",
        "## Import important libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For convenience. display all data values.\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read Dataset\n",
        "\n",
        "As outlined in the project description, it should be possible for the correctors to ecute your notebook without requiring any downloads.\n",
        "\n",
        "To facilitate access to the dataset without the need for downloads, use the data ovided in the public GitHub repository and provide a link to the raw version of the taset.\n",
        "\n",
        "The link to the raw version is as follows:\n",
        "\n",
        "*https://raw.githubusercontent.com/GITHUB_USERNAME/REPOSITORY_NAME/main/DATASETNAME.v*\n",
        "\n",
        "For example:\n",
        "\n",
        "[https://github.com/turcotte/csi4106-f24/blob/main/assignments-data/a1/01/glass.csv]ttps://github.com/turcotte/csi4106-f24/blob/main/assignments-data/a1/01/glass.csv)\n",
        "\n",
        "Now provide the link to YOUR dataset and read the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\supab\\AppData\\Local\\Temp\\ipykernel_10876\\315223141.py:31: DtypeWarning: Columns (26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dataset8 = pd.read_csv(url)\n"
          ]
        }
      ],
      "source": [
        "url = 'https://raw.githubusercontent.com/turcotte/csi4106-f24/main/assignments-data/a1/01/glass.csv'\n",
        "\n",
        "dataset1 = pd.read_csv(url)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/turcotte/csi4106-f24/refs/heads/main/assignments-data/a1/02/dermatology_database_1.csv'\n",
        "\n",
        "dataset2 = pd.read_csv(url)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/turcotte/csi4106-f24/refs/heads/main/assignments-data/a1/03/Maternal%20Health%20Risk%20Data%20Set.csv'\n",
        "\n",
        "dataset3 = pd.read_csv(url)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/turcotte/csi4106-f24/refs/heads/main/assignments-data/a1/04/car.data'\n",
        "\n",
        "dataset4 = pd.read_csv(url)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/turcotte/csi4106-f24/refs/heads/main/assignments-data/a1/05/WineQT.csv'\n",
        "\n",
        "dataset5 = pd.read_csv(url)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/turcotte/csi4106-f24/refs/heads/main/assignments-data/a1/06/16P.csv'\n",
        "\n",
        "dataset6 = pd.read_csv(url, encoding = 'latin1')\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/turcotte/csi4106-f24/refs/heads/main/assignments-data/a1/07/test.csv'\n",
        "\n",
        "dataset7 = pd.read_csv(url)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/turcotte/csi4106-f24/refs/heads/main/assignments-data/a1/07/train.csv'\n",
        "\n",
        "dataset8 = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Guidelines\n",
        "\n",
        "The following are the questions for Assignment 1. Under each question, we have provided an initial code cell. You are encouraged to add additional code cells to maintain logical separation of your code. For instance, place the definition of a function in one cell and its execution in a subsequent cell. This approach will help preserve clarity and enhance readability by avoiding the inclusion of excessive code within a single cell.\n",
        "\n",
        "1. **Analysis of Missing Values**: Examine the datasets to identify and assess ssing values in various attributes. Missing values may be represented by symbols ch as '?', empty strings, or other placeholders.\n",
        "\n",
        "    1.1 In the list of options, what are the datasets that contain missing values? ecifically, which attribute or attributes has missing values?\n",
        "\n",
        "    **Dataset 2 has errors in the \"age\" attribute.**\n",
        "    **Dataset 7 has many errors, in many attributes such as the Name, Age, SSN, Occupation and Annual Income attributes; these errors are found in both the test and train datasets.**\n",
        "\n",
        "    1.2 Describe the methodology used for this investigation, and provide the rresponding code.\n",
        "    \n",
        "    **Each dataset's columns are checked for errors. Errors vary from being empty values or NaN, but also are more specialized. Columns for numerical values are checked to have only numbers and not have non-numeric characters. Attributes that contain letters/words or alphanumeric strings are verified to see if the column either has entries that only belong to a set of already known values, or follow a certain formatted pattern. The code to check the datasets is provided in the cells below.**\n",
        "\n",
        "    1.3 Data imputation involves replacing missing or incomplete data with substituted values to preserve the dataset's integrity for subsequent analysis. Propose imputation strategies for each attribute with missing values.\n",
        "\n",
        "    **It is possible to query for a list of values that are missing/invalid, as shown in some code cells below. What can then be done is that for each missing/invalid cell, the average value across the valid entries is entered if it's a numeric value such as Age, so the average is preserved. For string data following a defined set of values (categorical data), the invalid sections could be set as the current median value of that column to preserve the median. In some cases, a default value could be entered so that the potential median/average is not tampered with, and can be recognized as a default value and discarded. In SSN, the value 000-000-0000 could be planted as a default value. In Occupation, the word \"Invalid\" could stand in for empty values, and then discarded when it comes to real processing of the data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 1 - Glass:\n",
            "Null/NaN Values: 0\n",
            "No non-numeric characters found.\n",
            "Inappropriate negative numbers: 0\n",
            "Data is clean; no missing or incorrect data\n"
          ]
        }
      ],
      "source": [
        "# Dataset 1\n",
        "print(\"Dataset 1 - Glass:\")\n",
        "nullValues = dataset1.isnull().sum().sum()\n",
        "print(\"Null/NaN Values:\", nullValues)\n",
        "\n",
        "nonNumericColumns = dataset1.apply(lambda x: pd.to_numeric(x, errors='coerce')).isnull().any().any()\n",
        "if nonNumericColumns == True:\n",
        "    print(\"Non-numeric characters found. Data has missing values.\")\n",
        "else: print(\"No non-numeric characters found.\")\n",
        "\n",
        "negativeValues = (dataset1.select_dtypes(include=['float64', 'int64']) < 0).sum().sum()\n",
        "print(\"Inappropriate negative numbers:\",negativeValues)\n",
        "\n",
        "if nullValues == 0 and negativeValues == 0 and nonNumericColumns == False:\n",
        "    print(\"Data is clean; no missing or incorrect data\")\n",
        "else: print(\"Some data is missing or incorrect.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Dataset 2 - Dermatology:\n",
            "Null/NaN Values: 0\n",
            "Non-numeric characters found. Data has missing values.\n",
            "erythema                               False\n",
            "scaling                                False\n",
            "definite_borders                       False\n",
            "itching                                False\n",
            "koebner_phenomenon                     False\n",
            "polygonal_papules                      False\n",
            "follicular_papules                     False\n",
            "oral_mucosal_involvement               False\n",
            "knee_and_elbow_involvement             False\n",
            "scalp_involvement                      False\n",
            "family_history                         False\n",
            "melanin_incontinence                   False\n",
            "eosinophils_infiltrate                 False\n",
            "PNL_infiltrate                         False\n",
            "fibrosis_papillary_dermis              False\n",
            "exocytosis                             False\n",
            "acanthosis                             False\n",
            "hyperkeratosis                         False\n",
            "parakeratosis                          False\n",
            "clubbing_rete_ridges                   False\n",
            "elongation_rete_ridges                 False\n",
            "thinning_suprapapillary_epidermis      False\n",
            "spongiform_pustule                     False\n",
            "munro_microabcess                      False\n",
            "focal_hypergranulosis                  False\n",
            "disappearance_granular_layer           False\n",
            "vacuolisation_damage_basal_layer       False\n",
            "spongiosis                             False\n",
            "saw_tooth_appearance_retes             False\n",
            "follicular_horn_plug                   False\n",
            "perifollicular_parakeratosis           False\n",
            "inflammatory_mononuclear_infiltrate    False\n",
            "band_like_infiltrate                   False\n",
            "age                                     True\n",
            "class                                  False\n",
            "dtype: bool\n",
            "Inappropriate negative numbers: 0\n",
            "Some data is missing or incorrect.\n"
          ]
        }
      ],
      "source": [
        "# Dataset 2\n",
        "print(\"\\n\\nDataset 2 - Dermatology:\")\n",
        "\n",
        "nullValues = dataset1.isnull().sum().sum()\n",
        "print(\"Null/NaN Values:\", nullValues)\n",
        "\n",
        "nonNumericColumns = dataset2.apply(lambda x: pd.to_numeric(x, errors='coerce')).isnull().any().any()\n",
        "if nonNumericColumns:\n",
        "    print(\"Non-numeric characters found. Data has missing values.\")\n",
        "    print(dataset2.apply(lambda x: pd.to_numeric(x, errors='coerce')).isnull().any())\n",
        "else: print(\"No non-numeric characters found.\")\n",
        "\n",
        "negativeValues = (dataset2.select_dtypes(include=['float64', 'int64']) < 0).sum().sum()\n",
        "print(\"Inappropriate negative numbers:\",negativeValues)\n",
        "\n",
        "if nullValues == 0 and negativeValues == 0 and nonNumericColumns == False:\n",
        "    print(\"Data is clean; no missing or incorrect data\")\n",
        "else: print(\"Some data is missing or incorrect.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Dataset 3 - Maternal Health Risk:\n",
            "Null/NaN Values in Numeric Columns: 0\n",
            "No non-numeric characters found in Numeric Columns.\n",
            "Inappropriate negative numbers in Numeric Columns: 0\n",
            "All values in RiskLevel are valid.\n",
            "Data is clean; no missing, incorrect, or invalid data.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\supab\\AppData\\Local\\Temp\\ipykernel_10876\\1411012348.py:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  nonNumericColumns = dataset3[numberColumns].applymap(lambda x: pd.to_numeric(x, errors='coerce')).isnull().any().any()\n"
          ]
        }
      ],
      "source": [
        "# Dataset 3\n",
        "\n",
        "print(\"\\n\\nDataset 3 - Maternal Health Risk:\")\n",
        "\n",
        "# Because this dataset has columns with text, we must segment the columns into numeric and the non-numeric.\n",
        "\n",
        "numberColumns = dataset3.columns[:-1]\n",
        "riskLevelColumn = 'RiskLevel'\n",
        "permittedRisks = [\"low risk\", \"mid risk\", \"high risk\"]\n",
        "\n",
        "# Numeric column stuff\n",
        "nullValues = dataset3[numberColumns].isnull().sum().sum()\n",
        "print(\"Null/NaN Values in Numeric Columns:\", nullValues)\n",
        "\n",
        "nonNumericColumns = dataset3[numberColumns].applymap(lambda x: pd.to_numeric(x, errors='coerce')).isnull().any().any()\n",
        "if nonNumericColumns:\n",
        "    print(\"Non-numeric characters found in Numeric Columns. Data has missing or incorrect values.\")\n",
        "else: print(\"No non-numeric characters found in Numeric Columns.\")\n",
        "\n",
        "negative_values = (dataset3[numberColumns].select_dtypes(include=['float64', 'int64']) < 0).sum().sum()\n",
        "print(\"Inappropriate negative numbers in Numeric Columns:\", negative_values)\n",
        "\n",
        "# Check risk level column for invalid values.\n",
        "invalidRiskLevels = ~dataset3[riskLevelColumn].isin(permittedRisks)\n",
        "invalidRiskCount = invalidRiskLevels.sum()\n",
        "if invalidRiskCount > 0:\n",
        "    print(f\"Invalid entries found in {riskLevelColumn}: {invalidRiskCount}\")\n",
        "    print(dataset3[invalidRiskLevels])  \n",
        "else: print(f\"All values in {riskLevelColumn} are valid.\")\n",
        "\n",
        "if nullValues == 0 and negative_values == 0 and not nonNumericColumns and invalidRiskCount == 0:\n",
        "    print(\"Data is clean; no missing, incorrect, or invalid data.\")\n",
        "else: print(\"Some data is missing, incorrect, or invalid.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Dataset 4 - Car:\n",
            "Buying Errors: False\n",
            "Maint Errors: False\n",
            "Doors Errors: False\n",
            "Persons Errors: False\n",
            "Lug_Boot Errors: False\n",
            "Safety Errors: False\n",
            "ClassValue Errors: False\n"
          ]
        }
      ],
      "source": [
        "# Dataset 4\n",
        "\n",
        "print(\"\\n\\nDataset 4 - Car:\")\n",
        "\n",
        "buying = [\"vhigh\", \"high\", \"med\", \"low\"]\n",
        "maint = [\"vhigh\", \"high\", \"med\", \"low\"]\n",
        "doors = [\"2\", \"3\", \"4\", \"5more\"]\n",
        "persons = [\"2\", \"4\", \"more\"]\n",
        "lug_boot = [\"small\", \"med\", \"big\"]\n",
        "safety = [\"low\", \"med\", \"high\"]\n",
        "classValue = [\"unacc\", \"acc\", \"good\" \"vgood\"]\n",
        "\n",
        "invalidBuying = not dataset4.iloc[:, 0].isin(buying).any()\n",
        "invalidMaint = not dataset4.iloc[:, 1].isin(maint).any()\n",
        "invalidDoors = not dataset4.iloc[:, 2].isin(doors).any()\n",
        "invalidPersons = not dataset4.iloc[:, 3].isin(persons).any()\n",
        "invalidLug_Boot = not dataset4.iloc[:, 4].isin(lug_boot).any()\n",
        "invalidSafety = not dataset4.iloc[:, 5].isin(safety).any()\n",
        "invalidClassValue = not dataset4.iloc[:, 6].isin(classValue).any()\n",
        "\n",
        "print(\"Buying Errors:\",invalidBuying)\n",
        "print(\"Maint Errors:\",invalidMaint)\n",
        "print(\"Doors Errors:\",invalidDoors)\n",
        "print(\"Persons Errors:\",invalidPersons)\n",
        "print(\"Lug_Boot Errors:\",invalidLug_Boot)\n",
        "print(\"Safety Errors:\",invalidSafety)\n",
        "print(\"ClassValue Errors:\",invalidClassValue)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Dataset 5 - Wine Quality:\n",
            "Null/NaN Values: 0\n",
            "No non-numeric characters found.\n",
            "Inappropriate negative numbers: 0\n",
            "Data is clean; no missing or incorrect data\n"
          ]
        }
      ],
      "source": [
        "# Dataset 5\n",
        "\n",
        "print(\"\\n\\nDataset 5 - Wine Quality:\")\n",
        "\n",
        "nullValues = dataset5.isnull().sum().sum()\n",
        "print(\"Null/NaN Values:\", nullValues)\n",
        "\n",
        "nonNumericColumns = dataset5.apply(lambda x: pd.to_numeric(x, errors='coerce')).isnull().any().any()\n",
        "if nonNumericColumns:\n",
        "    print(\"Non-numeric characters found. Data has missing values.\")\n",
        "    print(dataset5.apply(lambda x: pd.to_numeric(x, errors='coerce')).isnull().any())\n",
        "else: print(\"No non-numeric characters found.\")\n",
        "\n",
        "negativeValues = (dataset5.select_dtypes(include=['float64', 'int64']) < 0).sum().sum()\n",
        "print(\"Inappropriate negative numbers:\",negativeValues)\n",
        "\n",
        "if nullValues == 0 and negativeValues == 0 and nonNumericColumns == False:\n",
        "    print(\"Data is clean; no missing or incorrect data\")\n",
        "else: print(\"Some data is missing or incorrect.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Dataset 6 - 16 Personalities:\n",
            "Null/NaN Values in Numeric Columns: 0\n",
            "No non-numeric characters found in Numeric Columns.\n",
            "All values in Personality are valid.\n"
          ]
        }
      ],
      "source": [
        "# Dataset 6\n",
        "\n",
        "print(\"\\n\\nDataset 6 - 16 Personalities:\")\n",
        "\n",
        "numberColumns = dataset6.columns[:-1]\n",
        "personalityColumn = \"Personality\"\n",
        "permittedPersonalities = [\"ESTJ\", \"ENTJ\", \"ESFJ\", \"ENFJ\", \"ISTJ\", \"ISFJ\", \"INTJ\", \"INFJ\", \"ESTP\", \"ESFP\", \"ENTP\", \"ENFP\", \"ISTP\", \"ISFP\", \"INTP\", \"INFP\"]\n",
        "\n",
        "nullValues = dataset6[numberColumns].isnull().sum().sum()\n",
        "print(\"Null/NaN Values in Numeric Columns:\", nullValues)\n",
        "\n",
        "nonNumericColumns = dataset6[numberColumns].map(lambda x: pd.to_numeric(x, errors='coerce')).isnull().any().any()\n",
        "if nonNumericColumns:\n",
        "    print(\"Non-numeric characters found in Numeric Columns. Data has missing or incorrect values.\")\n",
        "else: print(\"No non-numeric characters found in Numeric Columns.\")\n",
        "\n",
        "invalidPersonalities = ~dataset6[personalityColumn].isin(permittedPersonalities)\n",
        "invalidPersonalityCount = invalidRiskLevels.sum()\n",
        "if invalidPersonalityCount > 0:\n",
        "    print(f\"Invalid entries found in {personalityColumn}: {invalidPersonalityCount}\")\n",
        "    print(dataset3[invalidPersonalities])  \n",
        "else: print(f\"All values in {personalityColumn} are valid.\")\n",
        "\n",
        "\n",
        "if(nullValues == 0 and nonNumericColumns and invalidPersonalityCount == 0):\n",
        "    print(\"All columns valid. Data is clean.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Dataset 7:\n",
            "All IDs follow the correct format.\n",
            "All customer IDs follow the correct format.\n",
            "All values in Months are valid.\n",
            "There are NaN entries in the Name column.\n",
            "There are inappropriate entries in Age.\n",
            "There are inappropriate entries in SSN.\n",
            "There are inappropriate entries in Occupation.\n",
            "Annual Income has invalid entries.\n"
          ]
        }
      ],
      "source": [
        "# Dataset 7\n",
        "\n",
        "print(\"\\n\\nDataset 7:\")\n",
        "\n",
        "# IDs 0\n",
        "\n",
        "pattern = r'^0x[a-fA-F0-9]{4,5}$'\n",
        "\n",
        "# test finding an invalid ID\n",
        "# dataset7.iloc[0, 0] = 'invalidID'  \n",
        "\n",
        "idColumnValidity = dataset7.iloc[:, 0].str.match(pattern)\n",
        "nonMatchingIds = dataset7.iloc[:, 0][~dataset7.iloc[:, 0].str.match(pattern)]\n",
        "\n",
        "if idColumnValidity.all():\n",
        "    print(\"All IDs follow the correct format.\")\n",
        "else:\n",
        "    print(\"Some IDs do not follow the correct format:\")\n",
        "    print(nonMatchingIds)  # Print rows with invalid IDs\n",
        "\n",
        "# Customer IDs 1\n",
        "\n",
        "pattern = r'^CUS_0x[a-fA-F0-9]{3,4}$'\n",
        "\n",
        "idColumnValidity = dataset7.iloc[:, 1].str.match(pattern)\n",
        "nonMatchingIds = dataset7.iloc[:, 1][~dataset7.iloc[:, 1].str.match(pattern)]\n",
        "\n",
        "if idColumnValidity.all():\n",
        "    print(\"All customer IDs follow the correct format.\")\n",
        "else:\n",
        "    print(\"Some customer IDs do not follow the correct format:\")\n",
        "    print(nonMatchingIds)  # Print rows with invalid IDs\n",
        "\n",
        "# Months 2\n",
        "permittedMonths = [\"September\", \"November\", \"October\", \"December\"]\n",
        "\n",
        "invalidMonths = ~dataset7.iloc[:,2].isin(permittedMonths)\n",
        "invalidMonthCount = invalidMonths.sum()\n",
        "if invalidMonthCount > 0:\n",
        "    print(f\"Invalid entries found in Months: {invalidMonthCount}\")\n",
        "    print(dataset3[invalidMonths])  \n",
        "else: print(f\"All values in Months are valid.\")\n",
        "\n",
        "# Name 3\n",
        "\n",
        "nanCheck = dataset7.iloc[:, 3].isnull().any()\n",
        "emptyCheck = (dataset7.iloc[:, 3] == '').any()\n",
        "\n",
        "if nanCheck:\n",
        "    print(\"There are NaN entries in the Name column.\")\n",
        "if emptyCheck:\n",
        "    print(\"There are empty entries in the Name column.\")\n",
        "if not nanCheck and not emptyCheck:\n",
        "    print(\"The Name column is clean, no missing data.\")\n",
        "\n",
        "# Age 4\n",
        "\n",
        "nanCheck = dataset7.iloc[:, 4].isnull().any()\n",
        "if nanCheck:\n",
        "    print(\"There are empty/NaN entries in the Name column.\")\n",
        "\n",
        "pattern = r'^\\d{1,3}$'\n",
        "\n",
        "if not dataset7.iloc[:, 4].str.match(pattern).all():\n",
        "    print(\"There are inappropriate entries in Age.\")\n",
        "else:\n",
        "    print(\"Age is clean.\")\n",
        "\n",
        "# SSN 5\n",
        "pattern = r'\\d{3}-\\d{3}-\\d{4}'\n",
        "\n",
        "if not dataset7.iloc[:, 5].str.match(pattern).all():\n",
        "    print(\"There are inappropriate entries in SSN.\")\n",
        "else:\n",
        "    print(\"SSN is clean.\")\n",
        "\n",
        "# Occupation 6\n",
        "if dataset7.iloc[:, 6].str.match(\"_______\").any():\n",
        "    print(\"There are inappropriate entries in Occupation.\")\n",
        "else:\n",
        "    print(\"Occupation is clean.\")\n",
        "\n",
        "# Annual Income 7\n",
        "eighthColumn = dataset7.iloc[:, 7]\n",
        "\n",
        "emptyCheck = eighthColumn.isnull().any() or (eighthColumn == '').any()\n",
        "negativeCheck = pd.to_numeric(eighthColumn, errors='coerce').lt(0).any()\n",
        "nonNumericCheck = pd.to_numeric(eighthColumn, errors='coerce').isnull().any()\n",
        "if emptyCheck or negativeCheck or nonNumericCheck:\n",
        "    print(\"Annual Income has invalid entries.\")\n",
        "else:\n",
        "    print(\"Annual Income is clean.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Dataset 8:\n",
            "All IDs follow the correct format.\n",
            "All customer IDs follow the correct format.\n",
            "All values in Months are valid.\n",
            "There are NaN entries in the Name column.\n",
            "There are inappropriate entries in Age.\n",
            "There are inappropriate entries in SSN.\n",
            "There are inappropriate entries in Occupation.\n",
            "Annual Income has invalid entries.\n"
          ]
        }
      ],
      "source": [
        "# Dataset 8\n",
        "\n",
        "print(\"\\n\\nDataset 8:\")\n",
        "\n",
        "# print(dataset8)\n",
        "\n",
        "# IDs 0\n",
        "\n",
        "pattern = r'^0x[a-fA-F0-9]{4,5}$'\n",
        "\n",
        "# test finding an invalid ID\n",
        "# dataset8.iloc[0, 0] = 'invalidID'  \n",
        "\n",
        "idColumnValidity = dataset8.iloc[:, 0].str.match(pattern)\n",
        "nonMatchingIds = dataset8.iloc[:, 0][~dataset8.iloc[:, 0].str.match(pattern)]\n",
        "\n",
        "if idColumnValidity.all():\n",
        "    print(\"All IDs follow the correct format.\")\n",
        "else:\n",
        "    print(\"Some IDs do not follow the correct format:\")\n",
        "    print(nonMatchingIds)  # Print rows with invalid IDs\n",
        "\n",
        "# Customer IDs 1\n",
        "\n",
        "pattern = r'^CUS_0x[a-fA-F0-9]{3,4}$'\n",
        "\n",
        "idColumnValidity = dataset8.iloc[:, 1].str.match(pattern)\n",
        "nonMatchingIds = dataset8.iloc[:, 1][~dataset8.iloc[:, 1].str.match(pattern)]\n",
        "\n",
        "if idColumnValidity.all():\n",
        "    print(\"All customer IDs follow the correct format.\")\n",
        "else:\n",
        "    print(\"Some customer IDs do not follow the correct format:\")\n",
        "    print(nonMatchingIds)  # Print rows with invalid IDs\n",
        "\n",
        "# Months 2\n",
        "permittedMonths = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\"]\n",
        "\n",
        "invalidMonths = ~dataset8.iloc[:,2].isin(permittedMonths)\n",
        "invalidMonthCount = invalidMonths.sum()\n",
        "if invalidMonthCount > 0:\n",
        "    print(f\"Invalid entries found in Months: {invalidMonthCount}\")\n",
        "    print(dataset3[invalidMonths])  \n",
        "else: print(f\"All values in Months are valid.\")\n",
        "\n",
        "# Name 3\n",
        "\n",
        "nanCheck = dataset8.iloc[:, 3].isnull().any()\n",
        "emptyCheck = (dataset8.iloc[:, 3] == '').any()\n",
        "\n",
        "if nanCheck:\n",
        "    print(\"There are NaN entries in the Name column.\")\n",
        "if emptyCheck:\n",
        "    print(\"There are empty entries in the Name column.\")\n",
        "if not nanCheck and not emptyCheck:\n",
        "    print(\"The Name column is clean, no missing data.\")\n",
        "\n",
        "# Age 4\n",
        "\n",
        "nanCheck = dataset8.iloc[:, 4].isnull().any()\n",
        "if nanCheck:\n",
        "    print(\"There are empty/NaN entries in the Name column.\")\n",
        "\n",
        "pattern = r'^\\d{1,3}$'\n",
        "\n",
        "if not dataset8.iloc[:, 4].str.match(pattern).all():\n",
        "    print(\"There are inappropriate entries in Age.\")\n",
        "else:\n",
        "    print(\"Age is clean.\")\n",
        "\n",
        "# SSN 5\n",
        "pattern = r'\\d{3}-\\d{3}-\\d{4}'\n",
        "\n",
        "if not dataset8.iloc[:, 5].str.match(pattern).all():\n",
        "    print(\"There are inappropriate entries in SSN.\")\n",
        "else:\n",
        "    print(\"SSN is clean.\")\n",
        "\n",
        "# Occupation 6\n",
        "if dataset8.iloc[:, 6].str.match(\"_______\").any():\n",
        "    print(\"There are inappropriate entries in Occupation.\")\n",
        "else:\n",
        "    print(\"Occupation is clean.\")\n",
        "\n",
        "# Annual Income 7\n",
        "eighthColumn = dataset8.iloc[:, 7]\n",
        "\n",
        "emptyCheck = eighthColumn.isnull().any() or (eighthColumn == '').any()\n",
        "negativeCheck = pd.to_numeric(eighthColumn, errors='coerce').lt(0).any()\n",
        "nonNumericCheck = pd.to_numeric(eighthColumn, errors='coerce').isnull().any()\n",
        "if emptyCheck or negativeCheck or nonNumericCheck:\n",
        "    print(\"Annual Income has invalid entries.\")\n",
        "else:\n",
        "    print(\"Annual Income is clean.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. **Select and familiarize yourself with a classification task:** Choose one of e provided datasets for further investigation. It is advisable to select a dataset ntaining a sufficiently large number of examples, ideally around 1,000, to ensure bust results when applying machine learning algorithms in the subsequent assignment.\n",
        "\n",
        "    2.1 What is the objective of the task? Is it intended for a specific plication? Do you possess expertise in this particular domain of application?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Answer:* I would use a **multi-class classification** scheme to try to highlight concentrated trends in the spread of car attributes. It might be shown that certain combinations of attribute values might determine the car's class. I do not possess expertise in this domain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. **Attribute Analysis**: \n",
        "\n",
        "    3.1 Determine which attributes lack informativeness and should be excluded to prove the effectiveness of the machine learning analysis. If all features are emed relevant, explicitly state this conclusion.\n",
        "\n",
        "    3.2 Examine the distribution of each attribute (column) within the dataset. Utilize histograms or boxplots to visualize the distributions, identifying any underlying patterns or outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3.1** All attributes are potentially informative. By my hypothesis, all attributes should contribute to the \"class\" attribute, and we should be able to see a link between combinations of the attributes and the resulting \"class\" attributes. It would be informative to see which attributes contribute to the overall class of a vehicle and which ones are qualitative but inconsequential.\n",
        "\n",
        "**3.2** Three cells below is a script that outputs the data two cells below. Every attribute except the target value \"class\" is spread between its possible values as evenly as can be. Attributes that have 4 different values are split into almost-perfect quarters, and those that have 3 different values are divided in thirds.\n",
        "\n",
        "It is also worth noting that since no data is missing or invalid in this dataset, there is no skewing or bias towards a certain attribute's value.\n",
        "\n",
        "It's clear that the data is as equalized as possible and there are no outliers or errors. So, visualizations for these attributes would be not very informative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**buying**\\\n",
        "vhigh: 431\\\n",
        "high: 432\\\n",
        "med: 432\\\n",
        "low: 432\\\n",
        "Total: 1727\n",
        "\n",
        "**maint**\\\n",
        "vhigh: 431\\\n",
        "high: 432\\\n",
        "med: 432\\\n",
        "low: 432\\\n",
        "Total: 1727 \n",
        "\n",
        "**doors**\\\n",
        "2: 431\\\n",
        "3: 432\\\n",
        "4: 432\\\n",
        "5more: 432\\\n",
        "Total: 1727 \n",
        "\n",
        "**persons**\\\n",
        "2: 575\\\n",
        "4: 576\\\n",
        "more: 576\\\n",
        "Total: 1727 \n",
        "\n",
        "**lug_boot**\\\n",
        "small: 575\\\n",
        "med: 576\\\n",
        "big: 576\\\n",
        "Total: 1727 \n",
        "\n",
        "**safety**\\\n",
        "low: 575\\\n",
        "med: 576\\\n",
        "high: 576\\\n",
        "Total: 1727 \n",
        "\n",
        "**class**\\\n",
        "unacc: 1209\\\n",
        "acc: 384\\\n",
        "good: 69\\\n",
        "vgood: 65\\\n",
        "Total: 1727"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "buying\n",
            "vhigh: 431\n",
            "high: 432\n",
            "med: 432\n",
            "low: 432\n",
            "Total: 1727 \n",
            "\n",
            "maint\n",
            "vhigh: 431\n",
            "high: 432\n",
            "med: 432\n",
            "low: 432\n",
            "Total: 1727 \n",
            "\n",
            "doors\n",
            "2: 431\n",
            "3: 432\n",
            "4: 432\n",
            "5more: 432\n",
            "Total: 1727 \n",
            "\n",
            "persons\n",
            "2: 575\n",
            "4: 576\n",
            "more: 576\n",
            "Total: 1727 \n",
            "\n",
            "lug_boot\n",
            "small: 575\n",
            "med: 576\n",
            "big: 576\n",
            "Total: 1727 \n",
            "\n",
            "safety\n",
            "low: 575\n",
            "med: 576\n",
            "high: 576\n",
            "Total: 1727 \n",
            "\n",
            "class\n",
            "unacc: 1209\n",
            "acc: 384\n",
            "good: 69\n",
            "vgood: 65\n",
            "Total: 1727 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "columnMapping = {\n",
        "    'vhigh': 'buying',\n",
        "    'vhigh.1': 'maint',\n",
        "    '2': 'doors',\n",
        "    '2.1': 'persons',\n",
        "    'small': 'lug_boot',\n",
        "    'low': 'safety',\n",
        "    'unacc': 'class'\n",
        "}\n",
        "\n",
        "attributeValues = {\n",
        "    \"buying\": [\"vhigh\", \"high\", \"med\", \"low\"],\n",
        "    \"maint\": [\"vhigh\", \"high\", \"med\", \"low\"],\n",
        "    \"doors\": [\"2\", \"3\", \"4\", \"5more\"],\n",
        "    \"persons\": [\"2\", \"4\", \"more\"],\n",
        "    \"lug_boot\": [\"small\", \"med\", \"big\"],\n",
        "    \"safety\": [\"low\", \"med\", \"high\"],\n",
        "    \"class\": [\"unacc\", \"acc\", \"good\", \"vgood\"]\n",
        "}\n",
        "\n",
        "# Iterate over the column mapping and print the distribution of attribute values\n",
        "for actualColName, attributeName in columnMapping.items():\n",
        "    print(attributeName)\n",
        "\n",
        "    valueCounts = dataset4[actualColName].value_counts()\n",
        "    possibleValues = attributeValues[attributeName]\n",
        "\n",
        "    total = 0\n",
        "\n",
        "    # Print the count for each possible attribute value\n",
        "    for value in possibleValues:\n",
        "        count = valueCounts.get(value, 0)\n",
        "        print(f\"{value}: {count}\")\n",
        "        total += count\n",
        "\n",
        "    # Total entries for this attribute\n",
        "    print(\"Total:\", total, \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. **Class Distribution Analysis**: Investigate the distribution of class labels within the dataset. Employ bar plots to visualize the frequency of instances for each class, and assess whether the dataset is balanced or imbalanced.\n",
        "\n",
        "\n",
        "Below I will display the results of the \"class\" variable. We can see that out of the 1209 entries of the dataset, \"unacceptable\" makes up more than half of the cars. As the class improves, so reduces the count of cars in such a class. \"acc\" values are at a sharp decrease, and \"good\" and \"v-good\" entries are nearly equal in their relative size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "       ╔════════════════════════════════════════════════════════════════════════════════════════════════════╗\n",
        "unacc  ╢████████████████████████████████████████████████████████████████████████████████████████████████████╟ 1209\n",
        "acc    ╢████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░╟ 384\n",
        "good   ╢██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░╟ 69\n",
        "v-good ╢█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░╟ 65\n",
        "       ╠════════════════════════════════════════════════════════════════════════════════════════════════════╣\n",
        "       0                                                                                                    1209\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. **Preprocessing**: \n",
        "\n",
        "    5.1 For numerical features, determine the best transformation to use. Indicate e transformation that seems appropriate and why. Include the code illustrating how  apply the transformation. For at least one attribute, show the distribution before d after the transformation. See [Preprocessing data](https://scikit-learn.org/able/modules/preprocessing.html).\n",
        "\n",
        "    5.2 For categorical features, show how to apply [one-hot encoding](https://ikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).  your dataset does not have categorical data, show how to apply the one-hot encoder  the label (target variable)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   vhigh_high  vhigh_low  vhigh_med  vhigh_vhigh  vhigh.1_high  vhigh.1_low  \\\n",
            "0       False      False      False         True         False        False   \n",
            "1       False      False      False         True         False        False   \n",
            "2       False      False      False         True         False        False   \n",
            "3       False      False      False         True         False        False   \n",
            "4       False      False      False         True         False        False   \n",
            "\n",
            "   vhigh.1_med  vhigh.1_vhigh   2_2    2_3    2_4  2_5more  2.1_2  2.1_4  \\\n",
            "0        False           True  True  False  False    False   True  False   \n",
            "1        False           True  True  False  False    False   True  False   \n",
            "2        False           True  True  False  False    False   True  False   \n",
            "3        False           True  True  False  False    False   True  False   \n",
            "4        False           True  True  False  False    False   True  False   \n",
            "\n",
            "   2.1_more  small_big  small_med  small_small  low_high  low_low  low_med  \\\n",
            "0     False      False      False         True     False    False     True   \n",
            "1     False      False      False         True      True    False    False   \n",
            "2     False      False       True        False     False     True    False   \n",
            "3     False      False       True        False     False    False     True   \n",
            "4     False      False       True        False      True    False    False   \n",
            "\n",
            "   unacc_acc  unacc_good  unacc_unacc  unacc_vgood  \n",
            "0      False       False         True        False  \n",
            "1      False       False         True        False  \n",
            "2      False       False         True        False  \n",
            "3      False       False         True        False  \n",
            "4      False       False         True        False  \n"
          ]
        }
      ],
      "source": [
        "# The full dataset is categorical. So, one-hot encoding will be applied to the full dataset.\n",
        "\n",
        "dataset4Encoded = pd.get_dummies(dataset4)\n",
        "\n",
        "print(dataset4Encoded.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. **Training and target data**: Set the Python variable `X` to designate the data and `y` to designate the target class. Make sure to select only the informative features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X:    vhigh_high  vhigh_low  vhigh_med  vhigh_vhigh  vhigh.1_high  vhigh.1_low  \\\n",
            "0       False      False      False         True         False        False   \n",
            "1       False      False      False         True         False        False   \n",
            "2       False      False      False         True         False        False   \n",
            "3       False      False      False         True         False        False   \n",
            "4       False      False      False         True         False        False   \n",
            "\n",
            "   vhigh.1_med  vhigh.1_vhigh   2_2    2_3    2_4  2_5more  2.1_2  2.1_4  \\\n",
            "0        False           True  True  False  False    False   True  False   \n",
            "1        False           True  True  False  False    False   True  False   \n",
            "2        False           True  True  False  False    False   True  False   \n",
            "3        False           True  True  False  False    False   True  False   \n",
            "4        False           True  True  False  False    False   True  False   \n",
            "\n",
            "   2.1_more  small_big  small_med  small_small  low_high  low_low  low_med  \n",
            "0     False      False      False         True     False    False     True  \n",
            "1     False      False      False         True      True    False    False  \n",
            "2     False      False       True        False     False     True    False  \n",
            "3     False      False       True        False     False    False     True  \n",
            "4     False      False       True        False      True    False    False  \n",
            "y: 0    unacc\n",
            "1    unacc\n",
            "2    unacc\n",
            "3    unacc\n",
            "4    unacc\n",
            "Name: unacc, dtype: object\n"
          ]
        }
      ],
      "source": [
        "X = dataset4Encoded.drop(columns=['unacc_acc', 'unacc_good', 'unacc_unacc', 'unacc_vgood'])\n",
        "\n",
        "y = dataset4['unacc']\n",
        "\n",
        "print(\"X:\", X.head())\n",
        "\n",
        "print(\"y:\", y.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. **Training and test sets**: Split the dataset into training and testing sets. Reserve 20% of data for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 1381 samples\n",
            "Testing set size: 346 samples\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=79)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------------------------------------------------------------------\n",
        "\n",
        "# References\n",
        "\n",
        "https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/ \\\n",
        "https://pandas.pydata.org/docs/reference/frame.html \\\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
        "\n",
        "\n",
        "## AI transcript\n",
        "**Hint:** To share a link to your colab notebook, click on \"share\" on the top right. Then, under *General access* , change *Restricted* to \"Anyone with the link\"."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
